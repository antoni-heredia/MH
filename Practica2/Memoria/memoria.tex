\documentclass[12pt,a4paper]{article}
\usepackage[spanish,es-tabla]{babel}

\usepackage[utf8]{inputenc} % Escribir con acentos, ~n...
\usepackage{eurosym} % s´ımbolo del euro
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\usepackage{listings}             % Incluye el paquete listing
\usepackage[cache=false]{minted}
\usepackage{graphics,graphicx, float} %para incluir imágenes y colocarlas
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage{multirow}
\usepackage{array}
\usepackage{diagbox}
\usepackage{algorithm}
\usepackage{algpseudocode}


\title{
\normalfont \normalsize 
\textsc{{\bf Metaheurísticas (2018-2019)} \\ Grado en Ingeniería Informática \\ Universidad de Granada} \\ [20pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
{\huge Práctica 2.b:}\\
Técnicas de Búsqueda basadas en Poblaciones
para el Problema del Aprendizaje de Pesos en
Característica \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
\includegraphics{images/logo.png}	
}

\author{Antonio Jesús Heredia Castillo\\DNI:xxxxxxxxxx\\a.heredia.castillo@gmail.com\\Grupo MH3: Jueves de 17:30h a 19:30h} % Nombre y apellidos

\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Muestra el Título
\newpage %inserta un salto de página
\tableofcontents % para generar el índice de contenidos
\listoftables
\newpage
\section{Practica 1.b}
\subsection{Descripción del problema}
En este problema nos encontramos con un conjunto de datos que previamente han sido clasificados. Estos datos tienen asociado un serie de características, cada una con un valor en concreto. Con estos datos y haciendo uso de diferentes algoritmos y caricaturistas queremos crear un modelo que nos permita clasificar de forma automática cualquier valor nuevo que tengamos. En este caso vamos a comprar el resultado de tres diferentes técnicas.\\\\
La primera técnica que usamos es usar un clasificador K-NN,  utilizando la técnica de buscar el vecino mas cercano. Este método busca simplemente el elemento que esta mas cercano a el y lo clasifica con la misma etiqueta que el.\\\\
La siguiente técnica que vamos a usar es el clasificador RELIEF este algoritmo usando  amigos mas cercanos y enemigos mas cercano obtiene un vector de pesos que se puede usar para un clasificador 1-NN para modificar la importancia a la hora de obtener la distancia del vecino mas cercano.\\\\
La ultima técnica utilizada es una BL, que modifica los valores del vector de pesos usando el operador de vecino por mutación normal, que consiste en coger un atributo y modificarlo por un valor de la normal.\\\\

Todos estas técnicas estarán descritas mas adelante.
\subsection{Descripción de los algoritmos comunes}
Para cargar los datos simplemente utilizo \textbf{scipy} \cite{spiz}, que tiene una función para cargar directamente los ficheros arff. Como lo carga en formato Pandas, los normalizo con \textbf{preprocessing}   de \textbf{sklearn} \cite{Baz}
 y los paso a array de Numpy. \\\\
Para los grupos de datos, he usado un $datos[F,C]$ donde F es la cantidad de datos y C la cantidad de atributos que tiene esa clase de datos:
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|c|}
		\hline 
			\backslashbox{Datos}{Atributos}& atributo0 & atributo1 & ... & atributoC \\ 
		\hline 
		dato0 &  &  &  &  \\ 
		\hline 
		dato1 &  &  &  &  \\ 
		\hline 
		... &  &  &  &  \\ 
		\hline 
		datoF &  &  &  &  \\ 
		\hline 
	\end{tabular} 
\end{table}
\subsection{Representación de los datos}
La representación para para las etiquetas es igual de sencilla en este caso se representa de la siguiente forma, $vector[F]$,  un vector de tamaño F donde cada etiqueta de una posición corresponde al  dato que se encuentra en la misma posición en el array de datos:
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		dato&  etiqueta \\
		\hline 
		dato0 & a   \\ 
		\hline 
		dato1 & b  \\ 
		\hline 
		... &    \\ 
		\hline 
		datoC & a  \\ 
		\hline 
	\end{tabular} 
\end{table}
Para representar los pesos que obtenemos tanto en RELIEF como en Local Search, usaremos tambien un vector, en este caso sera $pesos[C]$, donde $C$ sera el numero de atributos que tienen nuestros datos:
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|}
		\hline 
		peso&  valor \\
		\hline 
		peso0 & 0.22335   \\ 
		\hline 
		peso1 & 0.78335  \\ 
		\hline 
		... &    \\ 
		\hline 
		pesoF & 0.5436  \\ 
		\hline 
	\end{tabular} 
\end{table}
Por otro lado también tendremos datos estadísticos para valorar las soluciones. Para cada uno de los siguientes datos también existirá su correspondiente media de todas las ejecuciones. Estos son: \begin{itemize}
	\item tiempo inicial: Recogido antes de inicial la computación de los datos.
	\item tiempo final: Cuando acaba la computación de los datos
	\item tasa acierto: Porcentaje de aciertos que ha tenido una ejecución.
	\item tasa reducción: Porcentaje de pesos que son inferior a 0.2
	\item función objetivo: Función que intentaremos maximizar en cada clasificador.
\end{itemize}
\subsection{Ejecución de métodos de búsqueda}
La forma de ejecutar todos los algoritmos la realizo de la misma forma. Realizo un separación de los datos usando la técnica de validación cruzada de 5 iteraciones. Esto consiste en separar los datos de forma uniforme. Para ello estoy usando la biblioteca \textbf{StratifiedKFold} de \textbf{sklearn}\cite{Baz}\\
En el pseudocodigo del algoritmo \ref{alg:ejec} podemos ver  como se usa. Lo primero que se realiza es obtener las 5 particiones de datos diferentes. StratifiedKFold nos proporciona \textbf{k} grupos de datos. Cada grupo de datos esta formado dos partes una del 80\% y otra del 20\%. Es decir, la parte del test ira variando de un grupo a otro. En el algoritmo  se puede ver cuando obtenemos los tiempos, para cada ejecución. También recogemos los datos que usaremos en para realizar las comparaciones.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State tiempoInicio = getTiempo()
		\State tiempoAnterior = tiempoInicio
		\State particiones = obtenerParticiones(datos, etiquetas,k=5)
		\For{particion en particiones}
		\State datosEstadisticos = clasificador(particion)
		\State datosTotales += datosEstadisticos
		\State tiempoActual = getTiempo()
		\State tiempoAnterior = tiempoActual
		\State mostrarDatosPorPantalla(datosEstadisticos,tiempoActual-tiempoAnterior)
		\EndFor
		\State tiempoActual = getTiempo()
		\State mostrarDatosPorPantalla(datosTotales, tiempoActual-tiempoInicio)
	\end{algorithmic}
	\label{alg:ejec}
	\caption{Algoritom para ejecutar los metodos de clasificacion}
\end{algorithm}
\subsection{Evaluador de soluciones}
Para evaluar nuevos datos usando los pesos he usado un algoritmos que me ha cedido mi compañero de clase Antonio Molner Domenec. En el se hace uso de KDDTree, el cual genera un árbol con los vecinos. Realizando así una búsqueda de cual es el mas cercano. 
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State X = datos * pesos
		\State X = eliminaDespreciables(X)
		\State árbol = generoArbol(X)
		\State vecinos = arbol.consulta(nuevoDato)
		\State aciertos = media(clase(vecinos) == claseNuevoDato)
		\State reducción = media(pesos < 0.2)
		\State \Return aciertos, reducción, (aciertos+reducion)/2
	
	\end{algorithmic}
	\label{alg:ejec}
	\caption{Algoritmo para la evaluación de las soluciones}
\end{algorithm}


\subsection{Descripción de los métodos de búsqueda}
\subsection{Clasificador 1-NN}
Lo primero que hay que explicar en este apartado es el clasificador 1-NN. Ya que es la base. Para este  algoritmo tendremos que tener una función que nos devuelva la distancia entre  entre dos datos. En nuestro caso usaremos la distancia euclidea. Que viene definida por la siguiente ecuación, donde $e_{1} $ y $e_{2}$ es un vector con las características de cada elemento:

\[ d_{e}\left(e_{1}, e_{2}\right)=\sqrt{\sum_{i}\left(e_{1}^{i}-e_{2}^{i}\right)^{2}} \]
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State 	claseMinima = etiquetas.primera
		\State distanciaMinima = distancia(datos,datos.primero,nuevoDato)
		\For{indice = 0 \textbf{hasta} tamaño(datos)}
		\State distancia = distancia(datos,datos[indice],nuevoDato)
		\If{distancia \textit{<} distanciaMinima)}
		\State distanciaMinima = distancia
		\State claseMinima = etiquetas[indice]
		\EndIf % <- Here
		\EndFor
		
		\State \Return claseMinima
		
	\end{algorithmic}
	\label{alg:knn}
	\caption{Algoritmo 1-NN}
\end{algorithm}
Una vez que tenemos la función de distancia lo único que tenemos que realizar es un simple bucle en el que recorramos todos los datos, y busquemos con cual tiene menor distancia el  nuevo dato. Cada vez que encontramos una distancia menor la guardamos y esa sera la clase de nuestro nuevo elemento. Al terminar lo único que hacemos es devolver la clase de ese dato. \\\\

Una vez que tenemos el algoritmo que clasifica lo único que tendremos que realizar es bucle que vaya pasando nuevos datos al clasificador y comparar la solución que nos da. Si es igual a la etiqueta real del dato, tendremos un acierto mas. Por ultimo se devuelve la media de los datos y listo.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State 	aciertos = 0
		\For{indice, dato en datos\_test}
		
		\If{clasificador(datos,etiquetas,test) \textit{==} etiquetas\_test[indice]}
		\State aciertos++
		\EndIf % <- Here
		
		
		\EndFor
		
		\State \Return aciertos / tamaño(datos\_test)
		
	\end{algorithmic}
	\label{alg:knncount}
	\caption{Algoritmo para contar datos bien clasificados con 1-NN}
\end{algorithm}
\subsection{Relief}
El algoritmo \ref{alg:relief} de Relief tiene un funcionamiento muy sencillo. Lo primero que realizo es crear una matriz de distancia con sckit-learn\cite{Baz}. Esto lo realizo para  evitar tener que calcularla cada vez que evaluó un dato. Para crear el vector de pesos, tenemos que buscar el amigo y el enemigo mas cercano a cada dato. Para evitar realizar un bucle por cada, realizo una comparacion dentro del for que recorre buscando amigos y enemigos. La primera comparación busca que tengan diferente clase, si es asi y ademas tiene una distancia mejor que el mejor enemigo, ese se convierte en el mejor enemigo. La siguiente comparación busca que tengan la misma etiqueta, pero que sea un dato diferente, ademas miramos si la distancia entre ellos  y si es menor, ese dato se convierte en el mejor amigo.\\
Una vez que hemos encontrado el mejor amigo y enemigo, podemos modificar el vector de pesos según aparece en las transparencias, donde $e_i$ es el dato que estamos comparando y $e_e$ y $e_a$ son mejor enemigo y mejor amigo respectivamente:
\[ W=W+\left|e_{i}-e_{e}\right|-\left|e_{i}-e_{a}\right| \]
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State 	pesos = vector.zeros(tamaño(datos))
		\State matrizDistancias = crearMatrizDistancias(datos, datos)
		\For{indice, dato \textbf{en}  datos}
			\State mejorEnemigo = inf
			\State mejorAmigo = inf
			\State enemigoMasCercano = -1
			\State amigoMasCercano = -1
			\For{indice2, dato2 \textbf{en}  datos}
				\If{etiquetas[indice]  \textit{!=} etiquetas[indice2]}
					\If{matrizDistancias[indice,indice2] \textbf{less} mejorEnemigo}
						\State mejorEnemigo = matrizDistancias[indice,indice2]
						\State enemigoMasCercano = dato2
					\EndIf % <- Here
				\EndIf % <- Here
				\If{etiquetas[indice]  \textit{==} etiquetas[indice2] \textbf{and} }
					\If{matrizDistancias[indice,indice2] \textbf{less} mejorAmigo}
						\State mejorAmigo = matrizDistancias[indice,indice2]
						\State amigoMasCercano = dato2
					\EndIf % <- Here
				\EndIf % <- Here
			\EndFor
			\State pesos = pesos + abs(dato-enemigoMasCercano) - abs(dato - amigoMasCercano)
			
		\EndFor
		\State maxPeso=maximo(pesos)
		\For{indice \textbf{en}  pesos}
			\If{peso[indice]  \textbf{less} 0.2 }
				\State peso[indice] = 0
			\Else
				\State peso[indice] = peso[indice] / maxPeso
			\EndIf
		\EndFor
		\State \Return pesos
		
	\end{algorithmic}
	\label{alg:relief}
	\caption{Algoritmo RELIEF}
\end{algorithm}
\subsection{Búsqueda local}
Este algoritmo de búsqueda local se basa en realizar mutaciones aleatorias en los pesos del modelo. Para esto tiene un máximo de iteraciones de 15000 o que no se haya modificado el vector completo por 20 veces. Las mutaciones la realiza a cada atributo por separado y si existe una mejora, se queda con ese nuevo peso, si no lo mejora vuelve a los pesos anteriores. La mutación se realiza en base a la Normal centrada en 0 y el valor se coge de forma aleatoria. Este algoritmo tarda mucho mas en ejecutarse que los demás ya que tiene que realizar muchas mas evaluaciones de la solución y esto es un proceso costoso. 

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State pesos = generarPesosAleatorio()
		\State mejorEstadisticos = evaluar(pesos,datos,etiquetas)
		
		\State cantCaractarteristicas = tamaño(peso)
		\State cantEjecuciones = 0
		\State cantExplorados = 0
		
		\While{cantEjecuciones \textbf{less} MAX and cantExplorados \textbf{less}  (20*cantCaractarteristicas)} 
			\For{indice \textbf{hasta}  cantCaractarteristicas}
				\State cantExplorados += 1
				\State cantEjecuciones += 1
				\State pesosactual =  copia(pesos)
				\State Z =  random.normal(0.0, pow(0.3,2))
				\State pesosactual[indice] += Z
				\If{pesosactual[indice]  \textbf{great} 1 }
					\State pesosactual[y]=1
				\ElsIf{pesosactual[indice]\textbf{less}0} :
					\State pesosactual[y]=0
				\EndIf
				\If{ pesosactual[indice] \textbf{!=} W[y] and pesos[indice] \textbf{great} 0.2}
				\State estadisticos = evaluar(pesos,datos,etiquetas)
				\If{ estadisticos \textbf{great} mejorEstadisticos}
					\State mejorEstadisticos = estadisticos
					\State pesos =  copia(pesosactual)
					\State cantExplorados = 0
					\State \textbf{break}
				\EndIf
				\EndIf
			 \EndFor
		\EndWhile
		\State \Return pesos
	\end{algorithmic}
	\label{alg:bl}
	\caption{Algoritmo de busqueda local}
\end{algorithm}

Una vez que el algoritmo ha terminado de ejecutar se evaluara el resultado de los pesos generado con los datos de train y se obtendrán los datos estadísticos que se muestran.

\subsubsection{Resumen}
Viendo los datos podemos observar que el que mejor se comporta posiblemente sea el de 1-nn ya que obtiene mas tasa de acierto, y eso se puede a la forma en la que se encuentran nuestros datos. Ya que es posible que las clases están muy separadas entre si y con el vecino mas cercano sea suficiente para predecir la clase a la que pertenece.Ademas al ser un algoritmo lineal es muy rapido de realizar. \\\\
Por otro lado tenemos Relief y Busqueda local. Relief es cierto que es mucho mas rápido, pero no obstante los resultado de la búsqueda local son mejores. Esto se debe a que solo se muta si ha mejorado el valor de la tasa de agregación, obtenido así mejores resultado. Ademas con la mutación aleatoria, no cae en ningún mínimo local.
\section{Practica 2.b}

\subsection{Esquema de representación de soluciones empleado}

Para el esquema de representación para un cromosoma ha sido muy simple, tengo un vector con los diferentes pesos, que serán los diferentes genes

\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/pesos}
	\caption{}
	\label{fig:pesos}
\end{figure}

Por tanto la población sera un conjunto de cromosomas.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion}
	\caption{}
	\label{fig:poblacion}
\end{figure}

\subsection{Función objetivo}

La funcion objetivo en nuestro caso, es una media de dos factores. Primero la tasa de acierto que tiene nuestro clasificador y segundo la tasa de reducción conseguida. 
$$
Funcion objetivo = \frac{porcAciertos + porcReduccion}{2} \cdot 100
$$

Donde el porcentaje de aciertos es:
$$
 \frac{Correctos}{TotalDatos}
$$
y el porcentaje de reducción es:

$$
\frac{pesos < 0.2}{TotalDatos}
$$
\subsection{Algoritmo de selección en AGs y operadores de cruce y mutación}

Para el algoritmo de selección se utiliza el conocido ``torneo binario ''. Este torneo se basa en elegir dos padres de forma aleatoria, evaluarlos, mirar cual es el que mejor función objetivo tiene y añadir ese como padre. En esta practica tendremos dos tipos de torneos binarios uno para el esquema generacional y otro para el esquema estacionario.
\begin{algorithm}[H]
\begin{algorithmic}[1]
	\State 	mejores = []
	\State 	mejorValor = 0
	\For{indice = 0 \textbf{hasta} tamaño(poblacion)}
	\State primero = poblacion.getAleatorio
	\State segundo = poblacion.getAleatorio
	\If{$\mbox{funcionObjetivo(primero)}_a<\mbox{funcionObjetivo(segundo)}_b$ }
	\State mejorValor = funcionObjetivo(segundo)
	\State mejores.agregar(segundo) 
	\Else
	\State mejorValor = funcionObjetivo(primero)
	\State mejores.agregar(primero) 
	\EndIf % <- Here
	\EndFor
	
	\State \Return mejores
	
\end{algorithmic}
\label{alg:knn}
\caption{Torneo binario}
\end{algorithm}
\subsubsection{Operador de cruce BLX}
El operador de cruce BLX, nos recibe dos cromosomas, y devuelve otros dos cromosomas hijo apartir de los padres.
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State 	h1 = []
		\State 	h2 = []
		\For{indice = 0 \textbf{hasta} cantidadGenes}
		\State Cmax = maximo(cromosoma1[indice], cromosoma2[indice])
		\State Cmin = minimo(cromosoma1[indice], cromosoma2[indice])
		\State I = Cmax-Cmin
		\State h1.añadir(random(Cmin-0.3*I,Cmax+0.3*I))
		\State h2.añadir(random(Cmin-0.3*I,Cmax+0.3*I))
		\EndFor
		
		\State \Return h1,h2
		
	\end{algorithmic}
	\label{alg:knn}
	\caption{Cruce BLX}
\end{algorithm}

\subsubsection{Operador de cruce aritmetico}
Este algoritmo tambien devuelve dos cromosomas hijo, aunque estos son iguales entre si. 
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State c1 = 0.5 * cromosoma1 + 0.5 * cromosoma2
		\State c2 = 0.5 * cromosoma1 + 0.5 * cromosoma2
		\State \Return c1,c2
	\end{algorithmic}
	\label{alg:knn}
	\caption{Cruce aritmético}
\end{algorithm}
\subsubsection{Operador de mutación}
Para la mutación, primero calculo cuantos genes tengo que mutar. Y sabiendo cuantos son necesario, realizo mutaciones a genes de forma aleatoria. El valor de la mutación también es aleatorio, sacado de la normal con $\sigma = 0.3$.
\begin{algorithm}[H]
	\begin{algorithmic}[1]

		\For{indice = 0 \textbf{hasta} cantidadMutaciones}
		\State cromosoma = random(Poblacion)
		\State gen = random(cantGenes)  
		\State poblacion[cromosoma][gen] += normal(0,0.3)
		\If{$poblacion[cromosoma][gen]>1$ }
		\State poblacion[cromosoma][gen] = 1
		\EndIf % <- Here
		\If{$poblacion[cromosoma][gen]<0$ }
		\State poblacion[cromosoma][gen] = 0
		\EndIf % <- Here
		\EndFor	
	\end{algorithmic}
	\label{alg:knn}
	\caption{Mutación}
\end{algorithm}
\subsubsection{Elitismo}
El elitismo se basa en que si después de los cruces y las mutaciones, el mejor de la población anterior desaparece, remplazamos el peor de la nueva población por el mejor de la anterior. 
\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State encontrado = false
		\State peor = null
		\For{indice = 0 \textbf{hasta} tamaño(poblacion)}
		\If{$poblacion[indice] != mejor$}
		\State encontrado = true
		\EndIf % <- Here
		\If{$evaluar(poblacion[indice]) < evaluar(peor)$}
		\State peor = poblacion[indice]
		\EndIf % <- Here
		
		\EndFor	
		\If{!encontrado}
		\State poblacion[peor] = mejor
		\EndIf % <- Here	
\end{algorithmic}
	\label{alg:knn}
	\caption{Elitismo}
\end{algorithm}

\subsubsection{Algoritmo genéticos generacionales}
El esquema de los algoritmo genéticos generacionales es el siguiente. El cruce, lo pongo de forma generica, puede ser tanto BLX como aritmético. 

\begin{algorithm}[H]
	\begin{algorithmic}[1]
		\State poblacion = inicializamosDeFormaAleatoria()
		\State mejor = null
		\For{indice = 0 \textbf{hasta} cantidadEvalucaiones}
		\State poblacion = torneoBinarioTodos(poblacion)
		\State mejor = mejor(poblacion)
		\State poblacion = cruce(poblacion)
		\State poblacion = torneoBinario(poblacion)
		\State poblacion = remplazoPeorElitimsmo(poblacion,mejor)
		\EndFor
		\State mejor = null
		\For{indice = 0 \textbf{hasta} tamaño(poblacion)}
		\If{$evalucaion(poblacion[indice])>evalucaion(mejor)$ }
		\State mejor = poblacion[indice]
		\EndIf % <- Here
		\EndFor	
		\State \Return mejor
	\end{algorithmic}
	\label{alg:knn}
	\caption{Algoritmo Genetico generacional}
\end{algorithm}



\subsection{Experimentos}
\subsubsection{Clasificador 1-knn}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion1}
	\caption{}
	\label{fig:poblacion1}
\end{figure}
\subsubsection{Relief}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion2}
	\caption{}
	\label{fig:poblacion2}
\end{figure}
\subsubsection{Búsqueda local}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion3}
	\caption{}
	\label{fig:poblacion3}
\end{figure}
\subsubsection{AGG-BLX}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion5}
	\caption{}
	\label{fig:poblacion5}
\end{figure}

\subsubsection{AGG-AC}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion4}
	\caption{}
	\label{fig:poblacion4}
\end{figure}

\subsubsection{AGE-BLX}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion7}
	\caption{}
	\label{fig:poblacion7}
\end{figure}

\subsubsection{AGE-AC}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion6}
	\caption{}
	\label{fig:poblacion6}
\end{figure}
\subsubsection{AM-(10,1.0)}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion8}
	\caption{}
	\label{fig:poblacion8}
\end{figure}

\subsubsection{AM-(10,0.1)}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion9}
	\caption{}
	\label{fig:poblacion9}
\end{figure}

\subsubsection{AM-(10,0.1Mejores)}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion10}
	\caption{}
	\label{fig:poblacion10}
\end{figure}
\subsubsection{Resultados globales}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/poblacion11}
	\caption{}
	\label{fig:poblacion11}
\end{figure}

\clearpage
\begin{thebibliography}{H}
	\bibitem{spiz} \href{www.scipy.org}{SciPy},
	\textit{Biblioteca open source de herramientas y algoritmos matemáticos para Python } 
	\bibitem{Baz} \href{https://scikit-learn.org}{scikit-learn},
	\textit{Biblioteca para aprendizaje de máquina de software libre para el lenguaje de programación Python} 
\end{thebibliography}
\end{document}